{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a baseline FAQ-answering RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the data file\n",
    "faq_path = r\"D:\\Work\\Development\\RAG-agent\\data\\Anadea homework -Tee Customizer FAQ.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The question is the key, the answer is the value\n",
    "\n",
    "with open(faq_path, 'r') as file:\n",
    "    data = file.read()\n",
    "\n",
    "import re\n",
    "\n",
    "pattern = re.compile(r'Q:(.*)\\nA:(.*)')\n",
    "\n",
    "parsed_data = re.findall(pattern , data)\n",
    "\n",
    "doc = [{\"question\": q.strip(), \"answer\": a.strip()} for q, a in parsed_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Work\\Development\\RAG-agent\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# Initialize ChromaDB client (persistent storage in \"./chroma_db\")\n",
    "chroma_client = chromadb.PersistentClient(path=\"../chroma_db\")\n",
    "\n",
    "# Load embedding model\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using question embeddings as a key to answers\n",
    "# User enters query, this query is compared to all the embedded questions and we get the answer to the most similar question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored question embeddings with answer metadata in ChromaDB!\n"
     ]
    }
   ],
   "source": [
    "# Extract only questions for embedding, while storing answers in metadata\n",
    "questions = [item[\"question\"] for item in doc]\n",
    "metadatas = [{\"question\": item[\"question\"], \"answer\": item[\"answer\"]} for item in doc]\n",
    "\n",
    "# Store question embeddings in ChromaDB with answers in metadata\n",
    "faq_collection = Chroma.from_texts(questions, embedding_model, metadatas=metadatas, client=chroma_client, collection_name=\"faq_collection\")\n",
    "\n",
    "print(\"Stored question embeddings with answer metadata in ChromaDB!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'Custom t-shirts can be returned within 30 days of delivery, provided they are in their original condition.', 'question': 'What is your return policy for custom t-shirts?'}\n",
      "{'answer': 'You can track your order by logging into your account and viewing your order history.', 'question': 'How can I track my order?'}\n",
      "{'answer': 'Yes, you can upload your design or choose from our library of designs.', 'question': 'Can I add my own design to the t-shirt?'}\n",
      "{'answer': 'Our headquarters is located in Riga, Latvia, and we operate multiple production facilities to ensure efficient service and delivery.', 'question': 'Where is TeeCustomizer located?'}\n"
     ]
    }
   ],
   "source": [
    "# Load ChromaDB\n",
    "retriever = Chroma(client=chroma_client, collection_name=\"faq_collection\", embedding_function=embedding_model).as_retriever()\n",
    "\n",
    "def query_chroma_db(query):\n",
    "    results = retriever.invoke(query, k=1)  # Retrieve top 1 most relevant\n",
    "    if results:\n",
    "        return results[0].metadata  # Return the stored answer\n",
    "    return \"No relevant answer found.\"\n",
    "\n",
    "# Example Queries\n",
    "print(query_chroma_db(\"How do I return an item?\"))  # Should return: \"We accept returns within 30 days of purchase.\"\n",
    "print(query_chroma_db(\"Where is my order?\"))  # Should match the tracking query\n",
    "print(query_chroma_db(\"Can i make my personal clothes?\"))\n",
    "\n",
    "print(query_chroma_db(\"What is the name of my dog?\")) # Fails the test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem:\n",
    "- What if we don't have the actual answer to the question? Vector database will give irrelevant answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='05b62368-9718-4795-a824-bd29dbd627da', metadata={'answer': 'Yes, we offer a variety of design templates to help you get started.', 'question': 'Do you offer design templates?'}, page_content='Do you offer design templates?'),\n",
       " Document(id='72611e7c-cdd6-4502-b8c3-7e0f8e518f75', metadata={'answer': 'Yes, we offer a variety of design templates to help you get started.', 'question': 'Do you offer design templates?'}, page_content='Do you offer design templates?')]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\n",
    "    \"LangChain provides abstractions to make working with LLMs easy\",\n",
    "    k=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a baseline customization getter tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the data file\n",
    "customizations_path = r\"D:\\Work\\Development\\RAG-agent\\data\\Anadea homework - Tee Customizer Shirts.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(customizations_path, 'r', encoding=\"utf-8\") as file:\n",
    "    data = file.read()\n",
    "\n",
    "import re\n",
    "\n",
    "pattern = r\"(\\w+):\\n(-.+)+\"\n",
    "matches = re.findall(pattern, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Styles', '- Crew Neck'),\n",
       " ('Genders', '-Male'),\n",
       " ('Colors', '- White'),\n",
       " ('Sizes', '- XS, S, M, L, XL, XXL'),\n",
       " ('Options', '- Screen Printing')]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Genders: ['Male', 'Female', 'Unisex']\n",
      "\n",
      "Colors: ['White', 'Black', 'Blue', 'Red', 'Green', 'Custom Colors']\n",
      "\n",
      "Sizes: ['XS, S, M, L, XL, XXL']\n",
      "\n",
      "Printing Options: ['Screen Printing', 'Embroidery', 'Heat Transfer', 'Direct-to-Garment']\n"
     ]
    }
   ],
   "source": [
    "pattern = r\"^([\\w\\s]+):\\s*\\n((?:-\\s*.+\\n?)+)\"\n",
    "\n",
    "matches = re.findall(pattern, data, re.MULTILINE)\n",
    "\n",
    "# Parsing the results into a dictionary\n",
    "parsed_data = {}\n",
    "\n",
    "for category, options in matches:\n",
    "    options_list = re.findall(r\"-\\s*(.+)\", options)  # Extract individual options\n",
    "    parsed_data[category] = options_list\n",
    "\n",
    "# Output the structured data\n",
    "for category, options in parsed_data.items():\n",
    "    print(f\"{category}: {options}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Work\\Development\\RAG-agent\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Path to the data file\n",
    "faq_path = r\"D:\\Work\\Development\\RAG-agent\\data\\Anadea homework -Tee Customizer FAQ.txt\"\n",
    "# The question is the key, the answer is the value\n",
    "\n",
    "with open(faq_path, 'r') as file:\n",
    "    data = file.read()\n",
    "\n",
    "import re\n",
    "\n",
    "pattern = re.compile(r'Q:(.*)\\nA:(.*)')\n",
    "\n",
    "parsed_data = re.findall(pattern , data)\n",
    "\n",
    "doc = [{\"question\": q.strip(), \"answer\": a.strip()} for q, a in parsed_data]\n",
    "import chromadb\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# # Initialize ChromaDB client (persistent storage in \"./chroma_db\")\n",
    "# chroma_client = chromadb.PersistentClient(path=\"../chroma_db\")\n",
    "\n",
    "# Load embedding model\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "vector_store = Chroma(embedding_function=embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['9bbec5fc-a0e5-4b3b-9861-ac27540dc0d3',\n",
       " '9f730236-c138-4b7f-9f6a-db379a9a595d',\n",
       " 'e3385c4c-91c6-4d31-900c-7644058c70b5',\n",
       " 'b6e1006c-e150-4191-8bf3-bd6c4cb4e346',\n",
       " '09270cbd-c42e-413b-86b2-5a13586cde0e',\n",
       " '29bc255a-9598-48ba-a288-4d76bcb03458',\n",
       " '54196628-dee9-4a73-a8a1-c3b100584aee',\n",
       " '27bf8be1-eaf3-4218-9969-5aafe2b07988',\n",
       " 'f8354ba1-fba3-476a-9392-b64666b1fc16',\n",
       " 'daea841d-7fbb-4860-b6cb-9e1efcbed9c1',\n",
       " '9b0bed3a-312b-44eb-8473-d7597c6a46bc',\n",
       " '14788006-9a14-40ef-b8cf-bf38f4dfb147',\n",
       " 'c7819179-68af-4a66-a38d-edbe7365ccaa',\n",
       " '0dc72056-8ca3-454a-97c4-e4e51ce6455a',\n",
       " 'e408a9da-6539-46d6-9d48-7b17dab1feb2',\n",
       " '32c008e0-8d4e-4b5d-8cbb-740298b4e69c',\n",
       " '8062f9fc-34b4-443e-b8b4-946fd7f03eff',\n",
       " '5e0a9366-0a13-484e-b307-0bfc415dd456',\n",
       " 'afa20ea9-d0ae-4053-a34d-b7deea633327',\n",
       " 'e77795f5-9e53-4b71-976a-837742ae6f54',\n",
       " '914be579-34cb-4903-9a3d-0ce5b64531dc',\n",
       " 'ec24fe71-7824-4fe0-953c-9a3759ef00a3',\n",
       " 'f39c1f15-f632-4d6d-acbe-865fc5a56b75',\n",
       " 'd4cdc0e2-a912-4955-a571-6f794c0039a9',\n",
       " '08908dc8-5850-4e18-8900-87ebfdb717e8',\n",
       " 'a0f1cc95-938b-4210-9721-931bb5bc261f',\n",
       " '058d5f35-7345-446d-9e21-6998d6b89a20',\n",
       " '40962fc3-f2ed-4c9c-97d2-cf27ce1cb0ac',\n",
       " '9c12cce6-deb8-4576-8f7c-6f50071e7b9b',\n",
       " '610417a0-fb17-4167-92a6-a83a990dd73f',\n",
       " '5921f4db-4f53-4f3d-9ee6-9fab35a73f8b',\n",
       " '5305c488-2b1d-4d19-bae1-157afe5d92f5']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.add_texts([i[\"question\"] for i in doc], metadatas=doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Custom t-shirts can be returned within 30 days of delivery, provided they are in their original condition.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.similarity_search(query=\"How do I return an item?\", k=1)[0].metadata[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Work\\Development\\RAG-agent\\.venv\\Lib\\site-packages\\langsmith\\client.py:253: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: (question goes here) \n",
      "Context: (context goes here) \n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "example_messages = prompt.invoke(\n",
    "    {\"context\": \"(context goes here)\", \"question\": \"(question goes here)\"}\n",
    ").to_messages()\n",
    "\n",
    "assert len(example_messages) == 1\n",
    "print(example_messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Work\\Development\\RAG-agent\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Found an incompatible version of auto-gptq. Found version 0.3.0, but only version >= 0.4.99 are supported",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m model_name_or_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTheBloke/TinyLlama-1.1B-Chat-v0.3-GPTQ\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# To use a different branch, change revision\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# For example: revision=\"gptq-4bit-32g-actorder_True\"\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name_or_path, use_fast\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     12\u001b[0m system_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mYou are an assistant for question-answering tasks.\u001b[39m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124mUse the following pieces of retrieved context to answer the question.\u001b[39m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124mIf you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt know the answer, just say that you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt know.\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124mContext: (context goes here) \u001b[39m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124mAnswer:\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n",
      "File \u001b[1;32md:\\Work\\Development\\RAG-agent\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    570\u001b[0m )\n",
      "File \u001b[1;32md:\\Work\\Development\\RAG-agent\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:3611\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   3608\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3609\u001b[0m         config\u001b[38;5;241m.\u001b[39mquantization_config \u001b[38;5;241m=\u001b[39m quantization_config\n\u001b[1;32m-> 3611\u001b[0m     hf_quantizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoHfQuantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantization_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpre_quantized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_quantized\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3614\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3616\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3617\u001b[0m     hf_quantizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Work\\Development\\RAG-agent\\.venv\\Lib\\site-packages\\transformers\\quantizers\\auto.py:156\u001b[0m, in \u001b[0;36mAutoHfQuantizer.from_config\u001b[1;34m(cls, quantization_config, **kwargs)\u001b[0m\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    151\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown quantization type, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquant_method\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - supported types are:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    152\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(AUTO_QUANTIZER_MAPPING\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    153\u001b[0m     )\n\u001b[0;32m    155\u001b[0m target_cls \u001b[38;5;241m=\u001b[39m AUTO_QUANTIZER_MAPPING[quant_method]\n\u001b[1;32m--> 156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtarget_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Work\\Development\\RAG-agent\\.venv\\Lib\\site-packages\\transformers\\quantizers\\quantizer_gptq.py:50\u001b[0m, in \u001b[0;36mGptqHfQuantizer.__init__\u001b[1;34m(self, quantization_config, **kwargs)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_optimum_available():\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading a GPTQ quantized model requires optimum (`pip install optimum`)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01moptimum\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgptq\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GPTQQuantizer\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimum_quantizer \u001b[38;5;241m=\u001b[39m GPTQQuantizer\u001b[38;5;241m.\u001b[39mfrom_dict(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquantization_config\u001b[38;5;241m.\u001b[39mto_dict_optimum())\n",
      "File \u001b[1;32md:\\Work\\Development\\RAG-agent\\.venv\\Lib\\site-packages\\optimum\\gptq\\__init__.py:15\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# coding=utf-8\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Copyright 2023 HuggingFace Inc. team.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantizer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GPTQQuantizer, load_quantized_model\n",
      "File \u001b[1;32md:\\Work\\Development\\RAG-agent\\.venv\\Lib\\site-packages\\optimum\\gptq\\quantizer.py:52\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01maccelerate\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     47\u001b[0m         cpu_offload_with_hook,\n\u001b[0;32m     48\u001b[0m         load_checkpoint_and_dispatch,\n\u001b[0;32m     49\u001b[0m     )\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01maccelerate\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhooks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m remove_hook_from_module\n\u001b[1;32m---> 52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mis_auto_gptq_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mauto_gptq\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m autogptq_version\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mauto_gptq\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m exllama_set_max_input_length\n",
      "File \u001b[1;32md:\\Work\\Development\\RAG-agent\\.venv\\Lib\\site-packages\\optimum\\utils\\import_utils.py:254\u001b[0m, in \u001b[0;36mis_auto_gptq_available\u001b[1;34m()\u001b[0m\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 254\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m    255\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound an incompatible version of auto-gptq. Found version \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but only version >= \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mAUTOGPTQ_MINIMUM_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m are supported\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    256\u001b[0m     )\n",
      "\u001b[1;31mImportError\u001b[0m: Found an incompatible version of auto-gptq. Found version 0.3.0, but only version >= 0.4.99 are supported"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_name_or_path = \"TheBloke/TinyLlama-1.1B-Chat-v0.3-GPTQ\"\n",
    "# To use a different branch, change revision\n",
    "# For example: revision=\"gptq-4bit-32g-actorder_True\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "                                             device_map=\"auto\",\n",
    "                                             trust_remote_code=False,\n",
    "                                             revision=\"main\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "system_message = \"\"\"You are an assistant for question-answering tasks.\n",
    "Use the following pieces of retrieved context to answer the question.\n",
    "If you don't know the answer, just say that you don't know.\n",
    "Use three sentences maximum and keep the answer concise.\n",
    "Question: (question goes here) \n",
    "Context: (context goes here) \n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = \"How do i return the item?\"\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\\n*** Generate:\")\n",
    "\n",
    "answer = vector_store.similarity_search(query=prompt, k=1)[0].metadata[\"answer\"]\n",
    "context = f\"Context:::{answer}\"\n",
    "prompt += context\n",
    "\n",
    "prompt_template=f'''<|im_start|>system\n",
    "{system_message}<|im_end|>\n",
    "<|im_start|>user\n",
    "{prompt}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "'''\n",
    "\n",
    "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
    "\n",
    "output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\n",
    "print(tokenizer.decode(output[0]))\n",
    "\n",
    "# Inference can also be done using transformers' pipeline\n",
    "\n",
    "print(\"*** Pipeline:\")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    top_k=40,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "\n",
    "print(pipe(prompt_template)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building agents with Smolagents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the data file\n",
    "faq_path = r\"D:\\Work\\Development\\RAG-agent\\data\\Anadea homework -Tee Customizer FAQ.txt\"\n",
    "# The question is the key, the answer is the value\n",
    "\n",
    "with open(faq_path, 'r') as file:\n",
    "    data = file.read()\n",
    "\n",
    "import re\n",
    "\n",
    "pattern = re.compile(r'Q:(.*)\\nA:(.*)')\n",
    "\n",
    "parsed_data = re.findall(pattern , data)\n",
    "\n",
    "doc = [{\"question\": q.strip(), \"answer\": a.strip()} for q, a in parsed_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# # Initialize ChromaDB client (persistent storage in \"./chroma_db\")\n",
    "# chroma_client = chromadb.PersistentClient(path=\"../chroma_db\")\n",
    "\n",
    "# Load embedding model\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "faq_store = chromadb.PersistentClient(path=\"../chroma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ToolCallingAgent.__init__() missing 2 required positional arguments: 'tools' and 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m retrieve_faq_answer(query, faq_data)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Instantiate and use the agent\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m faq_agent \u001b[38;5;241m=\u001b[39m \u001b[43mFAQAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     26\u001b[0m     user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsk me a question: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: ToolCallingAgent.__init__() missing 2 required positional arguments: 'tools' and 'model'"
     ]
    }
   ],
   "source": [
    "from smolagents import ToolCallingAgent, tool\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "# Sample FAQ database\n",
    "faq_data = {\n",
    "    \"What is your return policy?\": \"You can return items within 30 days of purchase with a receipt.\",\n",
    "    \"How can I track my order?\": \"Use the tracking link provided in your confirmation email.\",\n",
    "    \"Do you offer international shipping?\": \"Yes, we ship to most countries worldwide.\",\n",
    "}\n",
    "\n",
    "# Function to find the best matching FAQ answer\n",
    "def retrieve_faq_answer(query, faq_data, threshold=70):\n",
    "    best_match, score = process.extractOne(query, faq_data.keys())\n",
    "    if score >= threshold:\n",
    "        return faq_data[best_match]\n",
    "    return \"I don't know.\"\n",
    "\n",
    "# Define the agent\n",
    "class FAQAgent(ToolCallingAgent):\n",
    "    def run(self, query):\n",
    "        return retrieve_faq_answer(query, faq_data)\n",
    "\n",
    "# Instantiate and use the agent\n",
    "faq_agent = FAQAgent()\n",
    "while True:\n",
    "    user_input = input(\"Ask me a question: \")\n",
    "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "        break\n",
    "    response = faq_agent.run(user_input)\n",
    "    print(\"Bot:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import util\n",
    "\n",
    "# Tokenize questions for BM25\n",
    "tokenized_corpus = [q.lower().split() for q in questions]\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "def hybrid_retrieval(user_question):\n",
    "    # Get dense retrieval (FAISS)\n",
    "    query_embedding = model.encode([user_question], convert_to_numpy=True)\n",
    "    D, I = index.search(query_embedding, k=3)  # Retrieve top-3 candidates\n",
    "\n",
    "    # Get sparse retrieval (BM25)\n",
    "    tokenized_query = user_question.lower().split()\n",
    "    bm25_scores = bm25.get_scores(tokenized_query)\n",
    "    bm25_best_idx = int(np.argmax(bm25_scores))\n",
    "\n",
    "    # Merge results (Choose the best match)\n",
    "    all_candidates = list(set([I[0][0], bm25_best_idx]))\n",
    "    best_idx = max(all_candidates, key=lambda idx: bm25_scores[idx] + 1/(D[0][0]+1e-5))\n",
    "    \n",
    "    return qa_pairs[best_idx][\"answer\"]\n",
    "\n",
    "# Example usage\n",
    "print(hybrid_retrieval(\"Who is the author of 1984?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
